<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="jlearning.cn">
<meta property="og:url" content="http://learning.com/page/2/index.html">
<meta property="og:site_name" content="jlearning.cn">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jlearning.cn">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://learning.com/page/2/"/>





  <title> jlearning.cn </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?f69738a703f95d25fd5649f5b3d51e51";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">jlearning.cn</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/21/about-final-in-java/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/21/about-final-in-java/" itemprop="url">
                  java中的final关键字
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-21T10:05:23+08:00">
                2017-04-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/21/about-final-in-java/" class="leancloud_visitors" data-flag-title="java中的final关键字">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="final修饰类"><a href="#final修饰类" class="headerlink" title="final修饰类"></a>final修饰类</h3><p>final修饰的类不能被继承</p>
<p>一个类不能同时用final和abstract修饰。</p>
<h3 id="final修饰方法"><a href="#final修饰方法" class="headerlink" title="final修饰方法"></a>final修饰方法</h3><p>表示方法不能在子类中被override。</p>
<h3 id="final修饰变量"><a href="#final修饰变量" class="headerlink" title="final修饰变量"></a>final修饰变量</h3><p>final修饰的变量表示只能被赋值一次。</p>
<ul>
<li>当修饰一个原生数据类型时，该生数据类型的值不能发生变化。</li>
<li>当修饰一个引用类型时，表示该引用类型不能再指向其他对象了，但是该引用所指向的对象的内容是可以发生变化的。</li>
</ul>
<p>final修饰一个成员变量（属性）时，必须要<em>显式初始化</em>。</p>
<ul>
<li>一种在变量声明的时候初始化</li>
<li>第二种在类的构造函数中对这个变量赋值</li>
</ul>
<blockquote>
<p>java中的局部变量，需要赋初试值。</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/19/《java并发编程实战》读书笔记（二）——对象的共享/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/19/《java并发编程实战》读书笔记（二）——对象的共享/" itemprop="url">
                  《java并发编程实战》读书笔记（二）——对象的共享
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-19T11:04:45+08:00">
                2017-04-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/19/《java并发编程实战》读书笔记（二）——对象的共享/" class="leancloud_visitors" data-flag-title="《java并发编程实战》读书笔记（二）——对象的共享">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="可见性"><a href="#可见性" class="headerlink" title="可见性"></a>可见性</h3><p>只要在某个线程中无法检测到重排序情况（即使在其他线程中可以很明显地看到该线程中的重排序），那么就无法确保线程中的操作将按照程序中指定的顺序来执行。</p>
<p>主线程首先写入a，然后在没有同步的情况下写入b。那么读线程看到的顺序可能与写入的顺序完全相反。</p>
<p>在没有同步的情况下，编译器、处理器以及运行时都可能对操作的执行顺序进行一些意想不到的调整。在缺乏足够同步的多线程程序中，相对内存操作的执行顺序进行判断，几乎无法得出正确的结论。</p>
<blockquote>
<p>只要有数据在多个线程之间共享，就使用正确的同步。</p>
</blockquote>
<h4 id="失效数据"><a href="#失效数据" class="headerlink" title="失效数据"></a>失效数据</h4><h4 id="非原子的64位操作"><a href="#非原子的64位操作" class="headerlink" title="非原子的64位操作"></a>非原子的64位操作</h4><p>最低安全性保证：当线程在没有同步的情况下读取变量时，可能会得到一个失效值，至少这个值是之前某个线程设置的值，而不是一个随机值。</p>
<p>对于非volatile类型的long和double变量，JVM允许将64位的读操作或写操作分解为两个32位操作。如果读操作和写操作在不同的线程中执行，那么很可能会读到某个值的高32位，和另一个值的低32位。</p>
<p>除非用volatile来声明它们。</p>
<h4 id="加锁与可见性"><a href="#加锁与可见性" class="headerlink" title="加锁与可见性"></a>加锁与可见性</h4><p>内置锁可以用于确保某个线程以一种可预测的方式来查看另一个线程的执行结果。</p>
<p>访问某个共享且可变的变量时，要求所有线程都在同一个锁上同步，以确保某个线程程写入该变量的值对于其他变量变量来说，都是可见的。</p>
<blockquote>
<p>加锁的含义不仅仅局限于互斥行为，还包括内存可见性。为了确保所有线程都能看到共享变量的最新值，所有执行读操作或者写操作的线程都必须在同一个锁上同步。</p>
</blockquote>
<h4 id="Volatile变量"><a href="#Volatile变量" class="headerlink" title="Volatile变量"></a>Volatile变量</h4><p>用于确保将变量的更新操作通知到其他线程。</p>
<p>当把变量生命为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存操作一起重排序。</p>
<p>不会被缓存再寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。</p>
<p>仅当volatile变量能简化代码的实现以及对同步策略的验证时，才使用它们。</p>
<p>加锁可以确保原子性和可见性，volatile只能确保可见性。</p>
<p>满足一下所有条件时，才使用volatile变量：</p>
<ul>
<li>对变量的写入操作不依赖变量当前的值（a++不行），或能确保只有单个线程更新变量的值。</li>
<li>该变量不会与其他状态变量一起纳入不变性条件中。</li>
<li>在访问变量时不需要加锁。</li>
</ul>
<h3 id="发布与逸出"><a href="#发布与逸出" class="headerlink" title="发布与逸出"></a>发布与逸出</h3><p>发布（Publish）：是对象能够在当前作用域之外的代码中使用。</p>
<p>e.g. 将一个指向该对象的引用保存到其他代码可以访问的地方、在某一个非私有的方法中返回该引用、将引用传递到其他类的方法中。</p>
<p>逸出（Escape）：当某个不应该发布的对象被发布。</p>
<p>外部方法（Alien）：行为并不完全由类来规定的方法。包括其他类中定义的方法以及类中可以被改写的方法。非<code>private</code> <code>final</code></p>
<p>当把一个对象传递给某个外部方法，就相当于发布了这个对象。</p>
<p>使用封装的原因：封装能使对程序的正确性进行分析变得可能，并使得无意中破坏设计约束条件变得更难。</p>
<h4 id="安全的对象构造过程"><a href="#安全的对象构造过程" class="headerlink" title="安全的对象构造过程"></a>安全的对象构造过程</h4><p>不要再构造过程中使<code>this</code>引用逸出。</p>
<blockquote>
<p>一个错误：在构造函数中启动一个线程。</p>
<p>当对象在其构造函数中创建一个线程时，无论是显示创建（通过将它传给构造函数），还是隐式创建（由于Thread或Runnable是该对象的一个内部类），this引用都会被新创建的额线程共享。在对象尚未完全构造之前，新的线程就可以看见他。</p>
</blockquote>
<p>如果想在构造函数中注册一个时间监听器或启动线程，那么可以使用一个私有的构造函数和一个公共的工厂方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SafeListener</span></span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> EventListener listener;</div><div class="line">  <span class="function"><span class="keyword">private</span> <span class="title">SafeListener</span><span class="params">()</span></span>&#123;</div><div class="line">    listener = <span class="keyword">new</span> EventListener()&#123;</div><div class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(Event e)</span></span>&#123;</div><div class="line">        doSomething(e);</div><div class="line">      &#125;</div><div class="line">    &#125;;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SafeListener <span class="title">newInstance</span><span class="params">(EventSource source)</span></span>&#123;</div><div class="line">    safeListener safe = <span class="keyword">new</span> SafeListener();</div><div class="line">    source.registerListener(safe.listener);</div><div class="line">    <span class="keyword">return</span> safe;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="线程封闭"><a href="#线程封闭" class="headerlink" title="线程封闭"></a>线程封闭</h3><p>一种避免使用同步的方式就是不共享数据。这种技术被称为线程封闭（Thread Confinement）</p>
<h4 id="Ad-hoc线程封闭"><a href="#Ad-hoc线程封闭" class="headerlink" title="Ad-hoc线程封闭"></a>Ad-hoc线程封闭</h4><p>维护线程封闭性的指责完全由程序来承担。</p>
<p>非常脆弱，尽量少用它。</p>
<h4 id="栈封闭"><a href="#栈封闭" class="headerlink" title="栈封闭"></a>栈封闭</h4><p>是线程封闭的一个特例，在栈封闭中，只能通过局部变量才能访问对象。</p>
<p>局部变量的固有属性之一就是封闭在执行线程中。他们位于执行线程的栈中，其他线程无法访问这个栈。</p>
<blockquote>
<p>局部变量：离开了他的域就访问不到</p>
</blockquote>
<p>在维持对象引用的栈封闭性时，程序员需要多做一些工作以确保被应用的对象不会逸出。</p>
<h4 id="ThreadLocal类"><a href="#ThreadLocal类" class="headerlink" title="ThreadLocal类"></a>ThreadLocal类</h4><p>能使线程中的某个值与保存值的对象关联起来。</p>
<p>ThreadLocal提供类get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本。因此，get总是返回由当前执行线程在调用set时设置的最新值。</p>
<p>通常用于防止对单实例变量（Singleton）或全局变量进行共享。</p>
<p>当某个频繁执行的操作需要一个临时对象，又希望避免在每次执行时都重新分配该临时对象。使用ThreadLocal。</p>
<p>这些特定于线程的值保存值唉Thread对象中，当线程终止后，这些值会作为垃圾回收。</p>
<blockquote>
<p>download movie</p>
</blockquote>
<h3 id="不变性"><a href="#不变性" class="headerlink" title="不变性"></a>不变性</h3><p>不可变对象一定是线程安全的。</p>
<p>对象不可变的条件：</p>
<ul>
<li>对象创建以后其状态就不能修改</li>
<li>对象的所有域都是final类型</li>
<li>对象是正确创建的（在创建期间，this引用没有逸出）</li>
</ul>
<h4 id="Final域"><a href="#Final域" class="headerlink" title="Final域"></a>Final域</h4><p>final域能确保初始化过程的安全性，从而可以不受限制地访问不可变对象，并在共享这些对象时无须同步。</p>
<h4 id="使用Volatile类型来发布不可变对象"><a href="#使用Volatile类型来发布不可变对象" class="headerlink" title="使用Volatile类型来发布不可变对象"></a>使用Volatile类型来发布不可变对象</h4><p>对于在访问和更新多个相关变量时出现的竞争条件问题，可以通过将这些变量全部保存在一个不可变对象中来消除。</p>
<p>当线程获得了对该对象的引用后，就不必担心另一个线程会修改对象的状态。</p>
<p>通过包含多个状态变量的容器对象来维持不变形条件，并使用一个volatile类型的引用来确保可见性，是的Volatile Cached Factorizer在没有显式的使用锁的情况下仍然是线程安全的。</p>
<h3 id="安全发布"><a href="#安全发布" class="headerlink" title="安全发布"></a>安全发布</h3><h4 id="不正确的发布：正确的对象被破坏"><a href="#不正确的发布：正确的对象被破坏" class="headerlink" title="不正确的发布：正确的对象被破坏"></a>不正确的发布：正确的对象被破坏</h4>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/17/《Java并发编程实战》读书笔记（一）——线程安全性/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/17/《Java并发编程实战》读书笔记（一）——线程安全性/" itemprop="url">
                  《Java并发编程实战》读书笔记（一）——并发简介与线程安全性
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-17T16:59:22+08:00">
                2017-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/17/《Java并发编程实战》读书笔记（一）——线程安全性/" class="leancloud_visitors" data-flag-title="《Java并发编程实战》读书笔记（一）——并发简介与线程安全性">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="并发编程简介"><a href="#并发编程简介" class="headerlink" title="并发编程简介"></a>并发编程简介</h3><p>线程允许在同一个进程中同时存在多个程序控制流。线程共享进程范围内的资源。</p>
<h4 id="线程的优势"><a href="#线程的优势" class="headerlink" title="线程的优势"></a>线程的优势</h4><ol>
<li>线程可以降低程序的开发和维护成本、提升复杂应用程序的性能。</li>
<li>将大部分的异步工作流转换成串行工作流，更好的模拟人类的工作方式。</li>
<li>降低代码的复杂度。</li>
<li>在GUI应用中，提高用户界面的响应灵敏度。</li>
<li>在服务器应用中，提升资源利用率以及系统吞吐量。</li>
<li>简化JVM的实现，垃圾收集器通常在一个或多个专门的线程中运行。</li>
</ol>
<h5 id="发挥多处理器的强大能力"><a href="#发挥多处理器的强大能力" class="headerlink" title="发挥多处理器的强大能力"></a>发挥多处理器的强大能力</h5><ol>
<li>多核处理器：多线程发挥多核的优势。</li>
<li>单核处理器：多线程可以使程序再IO阻塞期间继续运行。</li>
</ol>
<h5 id="建模的简单性"><a href="#建模的简单性" class="headerlink" title="建模的简单性"></a>建模的简单性</h5><p>如果程序只包含一种类型的任务，比包含多种不同类型任务的程序要易于编写。</p>
<p>为每种类型的任务分配一个专门的线程，将程序的执行逻辑与调度机制的细节，交替执行的操作，异步IO以及资源等待等问题分离开来。</p>
<h5 id="异步事件的简化处理"><a href="#异步事件的简化处理" class="headerlink" title="异步事件的简化处理"></a>异步事件的简化处理</h5><p>服务器程序为每一个客户端分配一个线程。</p>
<h5 id="响应更灵敏的用户界面"><a href="#响应更灵敏的用户界面" class="headerlink" title="响应更灵敏的用户界面"></a>响应更灵敏的用户界面</h5><h4 id="线程带来的风险"><a href="#线程带来的风险" class="headerlink" title="线程带来的风险"></a>线程带来的风险</h4><h5 id="安全性问题"><a href="#安全性问题" class="headerlink" title="安全性问题"></a>安全性问题</h5><h5 id="跳跃性问题"><a href="#跳跃性问题" class="headerlink" title="跳跃性问题"></a>跳跃性问题</h5><p>死锁、饥饿。</p>
<h5 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h5><p>上下文切换、同步机制会抑制某些编译器优化，使内存缓存区中的数据无效，增加共享内存总线的同步流量。</p>
<hr>
<h3 id="线程安全性"><a href="#线程安全性" class="headerlink" title="线程安全性"></a>线程安全性</h3><p>要编写线程安全的代码，核心在于要对<strong>状态访问操作</strong>进行管理。特别是对共享的（Shared）和可变的（Mutable）状态的访问。</p>
<p>Informally，对象的状态是指存储在<strong>状态变量</strong>（实例或者静态域）中的数据。对象的状态可能包括其他依赖对象的域。</p>
<p>共享表示变量可以由多线程同时访问。可变意味着其值在生命周期内可以发生变化。</p>
<p>Java中同步机制关键字是<code>synchronized</code>，以独占加锁的方式。</p>
<p>“同步”还包括：<code>volatile</code>类型的变量、显式锁（Explicit Lock）、原子变量。</p>
<p>访问某个变量的代码越少，越容易确保对变量的所有访问都实现正确同步。</p>
<p>Java没有要求强制要求<strong>将状态都封装在类</strong>中，开发人员可以将状态保存在某个公开的域，甚至公开的静态域中，或者提供一个<strong>对内部对象的公开引用</strong>。然而，封装有利于线程安全。</p>
<p>在任何情况下，只有当类中<strong>仅包含自己的状态</strong>时，线程安全类才是有意义的。</p>
<h4 id="什么是线程安全性"><a href="#什么是线程安全性" class="headerlink" title="什么是线程安全性"></a>什么是线程安全性</h4><p>正确性：某个类的行为与其规范完全一致。</p>
<p>良好的规范中通常会定义各种不变性条件（Invariant）来约束对象的状态，以及定义各种后验条件（postcondition</p>
<p>）来描述对象操作的结果。</p>
<p>线程安全性：当多个线程访问某个类时，这个类式中都能表现出正确的行为，那么就称这个类是线程安全的。</p>
<blockquote>
<p>当多个线程访问某个类时，不管运行时环境采用何种调度方式，或者这些线程将如何交替执行，并且在主调代码中不需要任何额外的同步或协同，这个类就能表现出正确的行为，那么久称这个类是线程安全的。</p>
</blockquote>
<p>无状态对象一定是线程安全的。</p>
<h4 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h4><p>由于不恰当的执行时序而出现不正确的结果：竞态条件（Race Condition）</p>
<h5 id="竞态条件"><a href="#竞态条件" class="headerlink" title="竞态条件"></a>竞态条件</h5><p>最常见的竞态条件类型就是“Check-Then-Act”</p>
<h5 id="延迟初始化中的竞态条件"><a href="#延迟初始化中的竞态条件" class="headerlink" title="延迟初始化中的竞态条件"></a>延迟初始化中的竞态条件</h5><p>延迟初始化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//NotThreadSate</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LazyinitRace</span></span>&#123;</div><div class="line">  <span class="keyword">private</span> ExpensiveObject instance = <span class="keyword">null</span>;</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> ExpensiveObject <span class="title">getInstance</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">if</span>(instance == <span class="keyword">null</span>)</div><div class="line">      instance = <span class="keyword">new</span> Expensiveobject();</div><div class="line">    <span class="keyword">return</span> instance;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果将UnsafeSequence用于某个持久化框架中生成对象的标识，那么两个不同的对象最终将获得相同的标识，这就违反了标识的<strong>完整性约束条件</strong>。</p>
<h5 id="复合操作"><a href="#复合操作" class="headerlink" title="复合操作"></a>复合操作</h5><p>使用一个现有的线程安全类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@ThreadSafe</span></div><div class="line"><span class="function"><span class="keyword">public</span> class CountingFactorizer implements <span class="title">Servlet</span><span class="params">()</span></span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> AtomicLong count = <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getCount</span><span class="params">()</span></span>&#123; <span class="keyword">return</span> count.get();&#125;</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">service</span><span class="params">(ServletRequest req,ServletResponse resp)</span></span>&#123;</div><div class="line">    BigInteger i = extractFromRequest(req);</div><div class="line">    BigInteger[] factors = factor(i);</div><div class="line">    count.incrementAndGet();</div><div class="line">    encodeIntoResponse(resp,factors);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当在无状态的类中添加一个状态时，如果改状态完全由线程安全的对象来管理，那么这个类仍然是线程安全的。</p>
<h4 id="加锁机制"><a href="#加锁机制" class="headerlink" title="加锁机制"></a>加锁机制</h4><p>当在不变形条件中涉及多个变量时，各个变量之间并不是彼此独立的，而是某个变量的值会对其他变量的值产生约束。因此更新一个变量时，需要在同一个原子操作中对其他变量同时进行更新。</p>
<p>要保持状态的一致性，就需要在单个原子操作中更新所有相关的状态变量。</p>
<h5 id="内置锁"><a href="#内置锁" class="headerlink" title="内置锁"></a>内置锁</h5><p>Java提供了一种内置的锁机制来支持原子性，Synchronized Block（同步代码块）。</p>
<p>同步代码块包括两个部分：</p>
<ul>
<li>一个作为锁的对象引用</li>
<li>一个作为由这个锁保护的代码块</li>
</ul>
<h5 id="重入"><a href="#重入" class="headerlink" title="重入"></a>重入</h5><p>“重入”意味着获取锁的操作的粒度是“线程”，而不是“调用”。</p>
<p>实现方法：为每个锁关联一个<em>计数值</em>和一个<em>所有者线程</em>。</p>
<p>重入避免了，子类改写父类的<code>synchronized</code>方法，然后再这个方法里调用父类的方法。每个方法在执行前都会获取父类上的锁，如果没有重入，那么会永远停留在子类调用父类方法中。</p>
<h4 id="用锁来保护状态"><a href="#用锁来保护状态" class="headerlink" title="用锁来保护状态"></a>用锁来保护状态</h4><p>通过锁来实现对共享状态的独占访问——&gt;确保状态的一致性。</p>
<p>如果用同步来协调对某个变量的访问，那么在访问这个变量的所有位置上都需要使用同步。而且在访问变量的所有位置上都要使用同一个锁。称这个状态是由这个锁保护的。</p>
<p><code>@GuardedBy</code>内置锁</p>
<p><strong>之所以每个对象都有一个内置锁，是为了免去显式地创建锁对象。</strong>你需要自行构造加锁协议或者同步策略来实现对共享状态的安全访问，并且在陈旭中自始至终的使用它们。</p>
<blockquote>
<p> 一种常见的加锁约定：</p>
<p>将所有的可变状态都封装在对象内部，通过对象的内置锁对所有访问可变状态的代码路径进行同步。</p>
<p>e.g. Vector和其他同步集合类</p>
<p>缺点：如果添加新的方法或者代码路径时，忘记使用了同步，那么这种加锁协议会很容易被破坏。</p>
</blockquote>
<p>对于每个包含多个变量的不变性条件，其中涉及的所有变量都需要由同一个锁来保护。</p>
<h4 id="活跃性和性能"><a href="#活跃性和性能" class="headerlink" title="活跃性和性能"></a>活跃性和性能</h4><blockquote>
<p> 不良并发（Poor Concurrency）：</p>
<p>可同时调用的数量，不仅受到可用处理资源的限制，还收到应用程序本身结构的限制。</p>
</blockquote>
<p>在获取和释放锁等操作都需要一定的开销，因此如果将同步代码块分解得过细，并不好。</p>
<p>需要在安全性（必须满足），简单性和性能上权衡。一定不要为了性能牺牲简单性，可能会破坏安全性。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/15/Tensorflow入门——深入MNIST分类的例子/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/15/Tensorflow入门——深入MNIST分类的例子/" itemprop="url">
                  Tensorflow入门——深入MNIST分类的例子
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-15T23:59:40+08:00">
                2017-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          
             <span id="/2017/04/15/Tensorflow入门——深入MNIST分类的例子/" class="leancloud_visitors" data-flag-title="Tensorflow入门——深入MNIST分类的例子">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>翻译自：<a href="https://www.tensorflow.org/get_started/mnist/mechanics" target="_blank" rel="external">https://www.tensorflow.org/get_started/mnist/mechanics</a></p>
<p>（文中有来自于原文的图片和链接，需要翻墙可以查看和访问）</p>
<h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>MNIST是一个机器学习中的分类问题。这个问题根据28*28像素的灰度手写数字图片，决定这些图片代表什么数字，从0到9.</p>
<p><img src="https://www.tensorflow.org/images/mnist_digits.png" alt="MNIST Digits"></p>
<h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p>在<code>tun_training()</code>方法的顶端，<code>input_data.read_data_sets()</code>函数会确保正确的数据已经下载到你的本地训练文件夹下，然后解压数据，返回一个<code>DataSet</code>矩阵实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)</div></pre></td></tr></table></figure>
<p>注意：<code>fake_data</code>标志被用来做单元测试，读者可以安全的忽视。</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>data_sets.train</td>
<td>55000个图片和标记，用来训练。</td>
</tr>
<tr>
<td>data_sets.validation</td>
<td>5000图片和标记，用来迭代确认训练精度</td>
</tr>
<tr>
<td>data_sets.test</td>
<td>10000图片和标记，用来最终测试训练精度</td>
</tr>
</tbody>
</table>
<h4 id="输入和占位符（Placeholders）"><a href="#输入和占位符（Placeholders）" class="headerlink" title="输入和占位符（Placeholders）"></a>输入和占位符（Placeholders）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,</div><div class="line">                                                       mnist.IMAGE_PIXELS))</div><div class="line">labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))</div></pre></td></tr></table></figure>
<p>在训练过程中，所有的图像和标签数据在每一步中都被切分成<code>batch_size</code>，和这些占位符ops相匹配。然后使用<code>feed_dict</code>传递给<code>sess.run()</code>函数。</p>
<h3 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h3><p>在为数据创建占位符之后，<code>mnist.py</code>文件根据三部模式创建图：<code>interence()</code>,<code>loss()</code>,<code>training()</code></p>
<ol>
<li><code>interence()</code>：建立一个图用来前向做预测。</li>
<li><code>loss()</code>添加到ingerence图中，去计算出损失量。</li>
<li><code>training()</code>天健到loss图中，去计算和应用梯度。</li>
</ol>
<p><img src="https://www.tensorflow.org/images/mnist_subgraph.png" alt="img"></p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p><code>ingerence()</code>函数创建的图返回预测的结果。</p>
<p>他使用图像占位符作为输入，然后在此基础上建立带有ReLU激活函数的全连接层，之后使用十个节点的线性层输出结果。</p>
<p>每一层都是在唯一的<code>tf.name_scope</code>下创建的，实在这个范围下创建的前缀。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden1'</span>):</div></pre></td></tr></table></figure>
<p>在定义的作用域（scope），权重和偏置量被用来在每一层上使用它们的shapes生成<code>tf.Variable</code>实例。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">weights = tf.Variable(</div><div class="line">    tf.truncated_normal([IMAGE_PIXELS, hidden1_units],</div><div class="line">                        stddev=<span class="number">1.0</span> / math.sqrt(float(IMAGE_PIXELS))),</div><div class="line">    name=<span class="string">'weights'</span>)</div><div class="line">biases = tf.Variable(tf.zeros([hidden1_units]),</div><div class="line">                     name=<span class="string">'biases'</span>)</div></pre></td></tr></table></figure>
<p>例如，这些在作用域<code>hidden1</code>下面被创建，权重变量的唯一名称将是：<code>hidden1/weights</code></p>
<p>每个变量都给予一个初始化操作，作为它们构建的一部分。</p>
<p>在一般情况下，权重使用<code>tf.truncated_normal</code>来初始化，并且给予它们一个二维tensor shape，第一个dim表示权重链接输入层的单元数，第二个dim表示权重链接层的输出层单元数。对于第一层，名叫<code>hidden1</code>，维数是<code>[IMAGE_PIXCELS,hidden1_units]，因为权重链接图形输入到hidden1层。</code>tf.truncated_normal`使用被给的平均数和标准差进行随机初始化。</p>
<p>偏置量使用<code>tf.zeros</code>初始化，去确保以零向量开始，他们的shape是一个这一层单元数的简单的数字。</p>
<p>这个图的三个基本操作，两个<code>tf.nn.relu</code>封装了隐藏层所需要的<code>tf.matmul</code>和一个额外的<code>tf.matmul</code>用来得到logits。每一个操作将<code>tf.Variable</code>实例连接到每一个占位符和前一层输出的tensors。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)</div><div class="line">hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)</div><div class="line">logits = tf.matmul(hidden2, weights) + biases</div></pre></td></tr></table></figure>
<p>最终，<code>logits</code>tensor将会返回输出。</p>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p><code>loss()</code>函数通过添加损失量的计算，进一步构建图。</p>
<p>首先<code>labels_placeholder</code>的值被转换为64-bit数。然后<code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>操作被添加去由<code>labels_placeholder</code>和<code>ingerence()</code>函数输出进行比较得到1-hot标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">labels = tf.to_int64(labels)</div><div class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">    labels=labels, logits=logits, name=<span class="string">'xentropy'</span>)</div></pre></td></tr></table></figure>
<p>使用<code>tf.reduce_mean</code>去求这个batch交叉熵值的平均数作为总损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(cross_entropy, name=<span class="string">'xentropy_mean'</span>)</div></pre></td></tr></table></figure>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p><code>training()</code>通过梯度下降法添加所需操作去最小化损失。</p>
<p>首先，从<code>loss()</code>函数得到损失值，使用<code>tf.summary.scalar</code>处理。这个操作生成摘要写入事件文件中当使用<code>tf.summary/FileWriter</code>。在这个例子中，他将会产生每一次损失函数的快照值，并写出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(<span class="string">'loss'</span>, loss)</div></pre></td></tr></table></figure>
<p>然后，我们实例化一个<code>tf.train.GradientDescentOptimize</code>来负责根据需要的学习速率应用梯度下降法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate)</div></pre></td></tr></table></figure>
<p>然后，我们生成一个单独变量去包含训练步数的计步器，而且<code>tf.train.Optimizer.minimize</code>操作被用来更新可训练的权重而且增加全局步数。这个操作通常被叫做<code>train_op</code>，必须在TensorFlow session中运行，去开始整个训练过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="keyword">False</span>)</div><div class="line">train_op = optimizer.minimize(loss, global_step=global_step)</div></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>只要图构建好了，<code>fully_connected_feed.py</code>中的代码可以控制循环进行训练和评价。</p>
<h4 id="The-Graph"><a href="#The-Graph" class="headerlink" title="The Graph"></a>The Graph</h4><p>在<code>tun_training()</code>函数的顶端，是一个python<code>with</code>命令，表示所有的构建操作都和默认的全局<code>tf.Graph</code>实例相关联。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Graph().as_default():</div></pre></td></tr></table></figure>
<p>一个<code>tf.Graph</code>是一个可以同时执行的操作的集合。大多数TensorFlow使用情况下，都需要依赖一个默认图。</p>
<p>使用多张图的更复杂的用法也是可能的，超出这个简单教程的范围。</p>
<h4 id="The-Sessioin"><a href="#The-Sessioin" class="headerlink" title="The Sessioin"></a>The Sessioin</h4><p>只要所有的构建准备已经完成，所有需要的操作都已经生成，一个<code>tf.session</code>就会被创建去运行这个图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div></pre></td></tr></table></figure>
<p>也可以用<code>with</code>命令产生Session，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div></pre></td></tr></table></figure>
<p>session实例没有参数，意味着使用默认的本地session。</p>
<p>创建session后，立即使用<code>tf.Sessioni.run</code>初始化所有的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure>
<p><code>tf.Session.run</code>方法会运行和传入参数相关的图的所有子集。在第一次调用中，<code>init</code>操作是一个仅仅包含变量初始化的<code>tf.group</code>。图的剩余部分不在这里运行，他们在下面的训练循环中运行。</p>
<h4 id="Train-Loop"><a href="#Train-Loop" class="headerlink" title="Train Loop"></a>Train Loop</h4><p>在初始化变量之后，训练开始。</p>
<p>用户的代码控制训练过的每一步进行，可以训练的最简单的循环是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(FLAGS.max_steps):</div><div class="line">    sess.run(train_op)</div></pre></td></tr></table></figure>
<p>但是，这个教程稍稍复杂。 把输入数据切成片为了每一步和前面生成的占位符相匹配。</p>
<h5 id="Feed-the-Graph"><a href="#Feed-the-Graph" class="headerlink" title="Feed the Graph"></a>Feed the Graph</h5><p>每一步中，代码都会生成一个feed dictionary，包含这一步训练例子集合，用他们代表的占位符操作作为键。</p>
<p>在<code>fill_feed_dict()</code>函数中，输入的<code>DataSet</code>被下一个<code>batch_size</code>大小的图片和标签集合所需要，tensors和占位符相匹配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,FLAGS.fake_data)</div></pre></td></tr></table></figure>
<p>一个python字典对象生成，使用占位符作为键，代表的feed tensors作为值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;</div><div class="line">    images_placeholder: images_feed,</div><div class="line">    labels_placeholder: labels_feed,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个字典被传递到sess.run()的feed_dict参数来提供这些训练步骤的输入例子。</p>
<h5 id="Check-the-Statue"><a href="#Check-the-Statue" class="headerlink" title="Check the Statue"></a>Check the Statue</h5><p>这段代码指定了两个值，在运行当中调用：<code>[train_op,loss]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(FLAGS.max_steps):</div><div class="line">    feed_dict = fill_feed_dict(data_sets.train,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    _, loss_value = sess.run([train_op, loss],</div><div class="line">                             feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p>因为有两个值需要获取，<code>sess.run()</code>返回一个有两项的元组。获取值列表中的每一个Tensor都对应着一个返回元组中的numpy数组，包含着这个训练步骤中的Tensor的值。因为操作<code>train_op</code>没有输出值，对应的返回元组是None。但是，损失tensor的值可能会编程NaN，如果在训练中模型发散，所以我们需要使用日志捕获这个值。</p>
<p>假设训练运行良好，没有NaNs，这个训练循环也会每100步打印出简单状态，去让用户知道训练的状态：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    <span class="keyword">print</span> <span class="string">'Step %d: loss = %.2f (%.3f sec)'</span> % (step, loss_value, duration)</div></pre></td></tr></table></figure>
<h5 id="Visualize-the-Status"><a href="#Visualize-the-Status" class="headerlink" title="Visualize the Status"></a>Visualize the Status</h5><p>为了使用TensorBoard输出事件文件，所有的摘要数据都被手机到一个Tensor中，在图的构建阶段。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p>在session被创建之后，<code>tf.summary.FileWriter</code>被实例化去写事件文件，这些文件包含了图本身和摘要数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)</div></pre></td></tr></table></figure>
<p>最终，事件文件会在每一次摘要被评价的时候被新的摘要数据更新，通过作者的<code>add_summary()</code>函数输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">summary_str = sess.run(summary, feed_dict=feed_dict)</div><div class="line">summary_writer.add_summary(summary_str, step)</div></pre></td></tr></table></figure>
<p>当事件文件被写，TensorBoard将会打开训练文件去展示摘要的值：</p>
<p><img src="https://www.tensorflow.org/images/mnist_tensorboard.png" alt="MNIST TensorBoard"></p>
<h5 id="Save-a-Checkpoint"><a href="#Save-a-Checkpoint" class="headerlink" title="Save a Checkpoint"></a>Save a Checkpoint</h5><p>为了产生一个检查点文件，可以在之后用来恢复模型，或者更进一步的训练和评价，我们实例化<code>tf.train.Saver</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div></pre></td></tr></table></figure>
<p>在训练循环中，<code>tf.train.Saver.save</code>方法将会定期的调用去写入检查点文件，用来存储当前所有训练变量的训练字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.save(sess, FLAGS.train_dir, global_step=step)</div></pre></td></tr></table></figure>
<p>在未来的某个时间点，训练可能使用<code>tf.train.Saver.restore</code>方法重新加载模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.restore(sess, FLAGS.train_dir)</div></pre></td></tr></table></figure>
<h3 id="评价模型"><a href="#评价模型" class="headerlink" title="评价模型"></a>评价模型</h3><p>每一千步，代码会尝试通过训练集和测试机评价模型。<code>do_eval()</code>函数会被调用三次，对于训练、验证、测试数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Training Data Eval:'</span></div><div class="line">do_eval(sess,</div><div class="line">        eval_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.train)</div><div class="line"><span class="keyword">print</span> <span class="string">'Validation Data Eval:'</span></div><div class="line">do_eval(sess,</div><div class="line">        eval_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.validation)</div><div class="line"><span class="keyword">print</span> <span class="string">'Test Data Eval:'</span></div><div class="line">do_eval(sess,</div><div class="line">        eval_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.test)</div></pre></td></tr></table></figure>
<h4 id="Build-the-Eval-Graph"><a href="#Build-the-Eval-Graph" class="headerlink" title="Build the Eval Graph"></a>Build the Eval Graph</h4><p>在进入训练循环之前，Eval操作应该被通过调用<code>evaluation()</code>函数创建。其和<code>loss()</code>函数有着相同的logits/labels参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eval_correct = mnist.evaluation(logits, labels_placeholder)</div></pre></td></tr></table></figure>
<p><code>evaluation()</code>函数仅仅生成一个<code>tf.nn.in_top_k</code>操作，可以自动的评价每个模型输出是否正确，如果真是标签可以再K个最有可能的预测中发现。在这个例子中，我们设定K为1，仅仅在正确标签的情况下才是对的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eval_correct = tf.nn.in_top_k(logits, labels, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h4 id="Eval-Output"><a href="#Eval-Output" class="headerlink" title="Eval Output"></a>Eval Output</h4><p>我们现在可以创建一个循环，输入<code>feed_dict</code>然后调用<code>sess.run()</code>通过<code>eval_correct</code>操作，去评价方法在给定数据集上的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(steps_per_epoch):</div><div class="line">    feed_dict = fill_feed_dict(data_set,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    true_count += sess.run(eval_correct, feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p><code>true_count</code>变量仅仅累加了<code>in_top_k</code>操作正确的预测。所以正确率可以通过简单得除以总的例子数量来获取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">precision = true_count / num_examples</div><div class="line">print(<span class="string">'  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f'</span> %</div><div class="line">      (num_examples, true_count, precision))</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/10/《深入理解Java虚拟机》读书笔记（三）——垃圾收集器与内存分配策略/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/10/《深入理解Java虚拟机》读书笔记（三）——垃圾收集器与内存分配策略/" itemprop="url">
                  《深入理解Java虚拟机》读书笔记（三）——垃圾收集器与内存分配策略
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-10T21:59:27+08:00">
                2017-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/10/《深入理解Java虚拟机》读书笔记（三）——垃圾收集器与内存分配策略/" class="leancloud_visitors" data-flag-title="《深入理解Java虚拟机》读书笔记（三）——垃圾收集器与内存分配策略">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>为什么要去了解垃圾收集：</p>
<blockquote>
<ol>
<li>当需要排查各种内存溢出、内存泄漏问题时。</li>
<li>当垃圾收集成为系统达到更高并发量的瓶颈时。</li>
</ol>
<p>我们就需要对这些自动化的技术实施必要的监控和调节</p>
</blockquote>
<h3 id="判断哪些对象不可能再被任何途径使用"><a href="#判断哪些对象不可能再被任何途径使用" class="headerlink" title="判断哪些对象不可能再被任何途径使用"></a>判断哪些对象不可能再被任何途径使用</h3><h4 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h4><blockquote>
<p>给对象添加一个引用计数器，每当有一个地方引用它时，就加1，引用失效时，就减1.为0的对象就是不可能再被使用的。</p>
</blockquote>
<p>很难解决对象之间的相互循环引用的问题。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">objA.instance = objB;</div><div class="line">objB.instance = objA;</div></pre></td></tr></table></figure>
<p>这两个对象已经不可能再被访问，但是他们因为互相引用着对方，导致引用计数都不为0.</p>
<h4 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h4><blockquote>
<p>通过一个称谓“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（就是从GC Roots到这个对象不可达），证明此对象是不可用的。</p>
</blockquote>
<p>在Java语言中，可作为GC Roots的对象包括下面几种：</p>
<ul>
<li><em>虚拟机栈</em>（<strong>栈帧中的本地变量表</strong>）中引用的对象。</li>
<li><em>本地方法栈</em>中JNI（Native方法）引用的对象。</li>
<li><em>方法区</em>中类静态属性引用的对象。</li>
<li><em>方法区</em>中常量引用的对象。</li>
</ul>
<h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p>JDK1.2以前，引用的定义很传统：</p>
<blockquote>
<p>如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。</p>
</blockquote>
<p>JDK1.2之后，将引用的概念进行了扩充，分为：</p>
<ul>
<li>强引用（Strong Reference）<ul>
<li>代码中普遍存在，类似于<code>Object obj = new Objext()</code></li>
<li>永远不会回收</li>
</ul>
</li>
<li>软引用（Soft Reference）<ul>
<li>描述还有用但并非必需的对象</li>
<li>在系统将要发生内存溢出之前，把这些对象列进回收范围进行二次回收。</li>
<li>使用<code>SoftReferencel</code>类来实现。</li>
</ul>
</li>
<li>弱引用（Weak Reference）<ul>
<li>强度比软引用更弱一点。</li>
<li>只能生存到下次垃圾收集发生之前，无论内存是否足够，都会回收掉只被弱引用关联的对象。</li>
<li>使用<code>WeakReference</code>类来实现。</li>
</ul>
</li>
<li>虚引用（Phantom Reference）<ul>
<li>一个对象是否有虚引用的存在不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。</li>
<li>为一个对象设置虚引用关联的唯一目的是能在这个对象被收集器回收时收到一个系统通知。</li>
<li>使用<code>PhantomReference</code>类来实现</li>
</ul>
</li>
</ul>
<h4 id="Java对象死亡过程"><a href="#Java对象死亡过程" class="headerlink" title="Java对象死亡过程"></a>Java对象死亡过程</h4><p>两次标记过程：</p>
<ol>
<li>可达性分析，发现没有与GC Roots相连接的引用链——第一次标记并且进行一次筛选。</li>
<li>筛选条件是此对象是否有必要执行<code>finalize()</code>方法——没有覆盖或者已经被虚拟机调用过了，都视为“没有必要执行”。</li>
<li>如果有必要执行，把这个对象放置在<code>F-Queue</code>的队列之中，在售后由一个虚拟机自动建立的、低优先级的Finalizer线程去触发这个方法，但并不承诺会等待它运行结束。</li>
<li>如果<code>finalize()</code>方法中重新与引用链上的任何一个对象建立关联，在第二次标记是将被移除出“即将回收”的集合。</li>
</ol>
<p>任何一个对象的<code>finalize()</code>方法都只会被系统自动调用一次，如果对象面临下一次回收，方法不会再次执行。</p>
<p><code>finalize()</code>方法，运行代价高昂，不确定性大，无法保证各个对象的调用顺序，不建议使用。</p>
<h4 id="回收方法区"><a href="#回收方法区" class="headerlink" title="回收方法区"></a>回收方法区</h4><p>主要回收两个部分内容：</p>
<ul>
<li>废弃的常量<ul>
<li>没有任何一个对象的值是这个常量，也没有其他地方引用了这个字面量。</li>
</ul>
</li>
<li>无用的类<ul>
<li>该类所有的实例都已经被回收</li>
<li>加载该类的ClassLoader已经被回收</li>
<li>该类对应的java.lang.class对象没有在任何地方被引用，无法在任何地方通过<strong>反射</strong>访问该类的方法。</li>
</ul>
</li>
</ul>
<h3 id="垃圾收集方法"><a href="#垃圾收集方法" class="headerlink" title="垃圾收集方法"></a>垃圾收集方法</h3><h4 id="标记—清除算法"><a href="#标记—清除算法" class="headerlink" title="标记—清除算法"></a>标记—清除算法</h4><ul>
<li>首先标记出所有需要回收的对象，在标记完成后统一回收。</li>
<li>不足：<ul>
<li>标记和清除两个过程效率都不高。</li>
<li>标记清除之后会产生大量不连续的内存碎片。以后需要分配较大的对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</li>
</ul>
</li>
</ul>
<h4 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h4><ul>
<li>当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。</li>
<li>更清晰的讲解：如果jvm使用了coping算法，一开始就会将可用内存分为两块，from域和to域， 每次只是使用from域，to域则空闲着。当from域内存不够了，开始执行GC操作，这个时候，会把from域存活的对象拷贝到to域,然后直接把from域进行内存清理。 来自：<a href="http://blog.csdn.net/linsongbin1/article/details/51668859" target="_blank" rel="external">http://blog.csdn.net/linsongbin1/article/details/51668859</a></li>
</ul>
<blockquote>
<p>coping算法一般是使用在新生代中，因为新生代中的对象一般都是朝生夕死的，存活对象的数量并不多，这样使用coping算法进行拷贝时效率比较高。不过jvm并不会根据新生代的特点，在应用coping算法时，并不是把内存按照1:1来划分的，这样太浪费内存空间了。一般的jvm都是8:1。</p>
<p> jvm将Heap 内存划分为新生代与老年代，又将新生代划分为Eden(伊甸园) 与2块Survivor Space(幸存者区) ,然后在Eden –&gt;Survivor Space 以及From Survivor Space 与To Survivor Space 之间实行Copying 算法。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20160615082256469" alt="copying算法"></p>
<blockquote>
<ol>
<li>当Eden去满的时候,会进行young gc,把还活着的对象拷贝到Survivor From区；、 </li>
<li>当Survivor 0区也慢了,则把活着的对象移到Survivor To区,Survivor From被清空 </li>
<li>当Eden区再次发生young gc的时候,直接把存活的对象复制到Survivor To区 </li>
<li>当Survivor To区也满的时候,又会再次把存活的对象复制到Survivor From区,如此交换15次(由JVM参数MaxTenuringThreshold决定,这个参数默认是15)，<strong>进入老年代。</strong></li>
<li>万一存活对象数量比较多，那么To域的内存可能不够存放，这个时候会借助老年代的空间。</li>
</ol>
</blockquote>
<h4 id="标记—整理算法"><a href="#标记—整理算法" class="headerlink" title="标记—整理算法"></a>标记—整理算法</h4><ul>
<li>复制算法在对象存活率较高时效率会变低。考虑到所有对象都100%存活的情况，在老年代一般不能直接选用这种算法。</li>
<li>让所有存活的对象都向一端移动。</li>
</ul>
<h4 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h4><ul>
<li>在新生代中，因为每次垃圾收集都有大批对象死去，所有用复制算法。</li>
<li>老年代对象存活率高，没有额外空间对它进行分配担保，就必须使用<code>标记—清理</code>或者<code>标记—整理</code>算法来进行回收。</li>
</ul>
<h3 id="HotSpot的算法实现"><a href="#HotSpot的算法实现" class="headerlink" title="HotSpot的算法实现"></a>HotSpot的算法实现</h3><h4 id="枚举根节点"><a href="#枚举根节点" class="headerlink" title="枚举根节点"></a>枚举根节点</h4><ul>
<li>在全局性的引用（常量和类静态属性）和执行上下文（栈帧中的本地变量表）中。</li>
<li>对执行时间敏感——GC进行时必须停顿所有Java执行线程。</li>
<li>HotSpot使用OopMap的数据结构，在类加载完成的时候，就把对象内什么偏移量上是什么类型的数据计算出来。</li>
<li>在JIT编译过程中，也会在特定的位置记录下栈和寄存器中哪些位置是引用。</li>
</ul>
<h4 id="安全点"><a href="#安全点" class="headerlink" title="安全点"></a>安全点</h4><ul>
<li>OopMap<strong>内容变化的指令</strong>非常多，如果为每一条指令都生成对应的OopMap，将会需要大量的额外空间。</li>
<li>在特定位置生成OopMap,这些位置成为安全点。</li>
<li>安全点选定以程序“是否具有让程序长时间执行的特征”为标准进行选定的。<ul>
<li>方法调用</li>
<li>循环跳转</li>
<li>异常跳转</li>
</ul>
</li>
<li>如何在GC发生时，让所有线程（不包括执行JNI调用的线程）都“跑”到最近的安全点上再停顿下载。<ul>
<li>抢先式中断</li>
<li>主动式中断——简单地设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。</li>
</ul>
</li>
</ul>
<h4 id="安全区域"><a href="#安全区域" class="headerlink" title="安全区域"></a>安全区域</h4><ul>
<li>如果线程处于Sleep或者Blocked状态，无法响应JVM的中断请求。</li>
<li>安全区域指在一段代码片段中，引用关系不会发生变化。在这个区域中的任何地方开始GC都是安全的。</li>
<li>在线程执行到Safe Region中的代码时，首先标识。离开时，检查系统是否已经完成了根节点枚举，如果完成就继续执行，否则等待直到收到可以安全离开Sage Region的信号为止。</li>
</ul>
<h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h4><ul>
<li>以获取最短回收停顿时间为目标的收集器，尤其重视服务的响应速度。</li>
<li>基于标记—清楚算法实现</li>
<li>分为四个步骤：<ol>
<li>初始标记（Stop the World）标记一下GC Roots能直接关联到的对象</li>
<li>并发标记——GC Roots Tracing</li>
<li>重新标记（Stop the World）修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象标记记录</li>
<li>并发清除</li>
</ol>
</li>
<li>缺点：<ul>
<li>对CPU资源非常敏感</li>
<li>无法处理浮动垃圾（Floating Garbage）</li>
<li>标记—清楚算法会有大量空间碎片产生</li>
</ul>
</li>
</ul>
<h4 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h4><p>特点：</p>
<ul>
<li>并行与并发</li>
<li>分代收集</li>
<li>空间整合</li>
<li>可预测的停顿——能建立可预测的停顿时间模型，能然给使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾手机上的时间不得超过N毫秒。</li>
</ul>
<p>步骤：</p>
<ol>
<li>初试标记——标记一下GC Roots能直接关联到的对象，并修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象。</li>
<li>并发标记——从GC Roots开始对堆中对象进行可达性分析，可与用户程序并发执行。</li>
<li>最终标记——修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录。记录在Remembered Set Logs里面，合并到Remembered Set中。</li>
<li>筛选回收——对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来指定回收计划。</li>
</ol>
<h3 id="内存分配与回收策略"><a href="#内存分配与回收策略" class="headerlink" title="内存分配与回收策略"></a>内存分配与回收策略</h3><ul>
<li>对象主要分配在新生代的Eden区上，如果启动了本地线程分配缓冲，将按线程优先在TLAB上分配。</li>
<li>少数情况下也可能会直接分配在老年代中。</li>
</ul>
<h4 id="对象优先在Eden分配"><a href="#对象优先在Eden分配" class="headerlink" title="对象优先在Eden分配"></a>对象优先在Eden分配</h4><ul>
<li>大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，去你急将发起一次Minor GC。</li>
</ul>
<h4 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h4><ul>
<li>大对象指需要大量连续内存空间的Java对象。</li>
<li>最典型的大对象就是那种很长的字符串和数组。</li>
</ul>
<h4 id="长期存活的对象将进入老年代"><a href="#长期存活的对象将进入老年代" class="headerlink" title="长期存活的对象将进入老年代"></a>长期存活的对象将进入老年代</h4><h4 id="动态对象年龄判定"><a href="#动态对象年龄判定" class="headerlink" title="动态对象年龄判定"></a>动态对象年龄判定</h4><ul>
<li>如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于改年龄的对象就可以直接进入老年代。</li>
</ul>
<h4 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h4><ul>
<li>在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续孔家您是否大于新生代所有对象总空间，如果成立Minor GC可以确保是安全的。</li>
<li>如果不成立，会查看设置是否允许担保失败，如果允许那么会继续检查老年代最大可用的连续空间是否大于历次今生到老年代对象的平均大小，如果大于尝试进行一次Minor GC。</li>
<li>如果小于，或者设置不允许冒险，改为进行一次Full GC。</li>
</ul>
<blockquote>
<p> 从年轻代空间（包括 Eden 和 Survivor 区域）回收内存被称为 Minor GC。</p>
<p> Full GC 是清理整个堆空间—包括年轻代和永久代。</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/09/《深入理解java虚拟机》读书笔记（一）——关于Java/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/09/《深入理解java虚拟机》读书笔记（一）——关于Java/" itemprop="url">
                  《深入理解java虚拟机》读书笔记（一）——关于Java
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-09T23:03:46+08:00">
                2017-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/09/《深入理解java虚拟机》读书笔记（一）——关于Java/" class="leancloud_visitors" data-flag-title="《深入理解java虚拟机》读书笔记（一）——关于Java">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="走近java"><a href="#走近java" class="headerlink" title="走近java"></a>走近java</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Java的优点：</p>
<ol>
<li>摆脱硬件平台的束缚，一次编写处处运行。</li>
<li>提供了一个相对安全的内存管理和访问机制，避免了内存泄漏和指针越界。</li>
<li><strong>实现了热点代码检测和运行时的编译及优化，使java能随着运行时间的增加获得更高的性能</strong>。（通过执行计数器找到最具有编译价值的代码）</li>
<li>有一套完善的<strong>应用程序接口</strong>，无数来自商业机构和开源社区的第三方类库。</li>
</ol>
<h3 id="Java技术体系"><a href="#Java技术体系" class="headerlink" title="Java技术体系"></a>Java技术体系</h3><ul>
<li>把Java程序设计语言、Java虚拟机、Java API类库统称为JDK（Java Development Kit）。——最小开发环境。</li>
<li>可以吧Java API类库中的<strong>Java SE API</strong>（去掉工具API？）和Java虚拟机这两部分统称为JRE（Java Runtime Environment）。——标准运行环境。</li>
<li>根据技术所服务的领域划分，重点业务领域划分：<ul>
<li>Java Card：支持Java小程序（Applets）运行在小内存设备上。</li>
<li>Java Me（Mircro Edition）:移动终端平台。</li>
<li>Java SE（Standard Edition）：支持桌面级应用的平台。</li>
<li>Java EE（Enterprise Edition）：支持使用多层架构的企业应用（ERP，<strong>CRM</strong>）的Java平台，在SE基础上做了大量扩充和相关的部署支持。（扩充以javax.*作为包名）</li>
</ul>
</li>
</ul>
<p>### </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/09/《深入理解java虚拟机》读书笔记（二）——Java内存/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/09/《深入理解java虚拟机》读书笔记（二）——Java内存/" itemprop="url">
                  《深入理解java虚拟机》读书笔记（二）——Java内存
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-09T23:03:10+08:00">
                2017-04-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/09/《深入理解java虚拟机》读书笔记（二）——Java内存/" class="leancloud_visitors" data-flag-title="《深入理解java虚拟机》读书笔记（二）——Java内存">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Java内存"><a href="#Java内存" class="headerlink" title="Java内存"></a>Java内存</h2><p>从观念上介绍Java内存的各个区域，讲解这些区域的作用、服务对象以及其中可能产生的问题。</p>
<h3 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h3><p><img src="http://images.cnblogs.com/cnblogs_com/Cratical/201208/201208212311259458.png" alt="运行时数据区"></p>
<h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><ul>
<li>当前线程所执行的字节码的行号指示器。</li>
<li>每条线程需要一个独立的程序计数器，各条线程之间计数器互不影响，独立存储。所以，这类内存区域为“线程私有”的内存。</li>
<li>此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。</li>
<li>若执行Java方法，记录正在执行的虚拟机字节码指令的地址。</li>
<li>若执行<strong>Native方法</strong>，计数器值为空（Undefined）。</li>
</ul>
<h4 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h4><ul>
<li>和程序计数器一样，也是线程私有的，生命周期和线程相同。</li>
<li>虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储<strong>局部变量表、操作数栈、动态链接、方法出口</strong>等信息。方法的调用到完成——栈帧在虚拟机栈中入栈到出栈。</li>
<li><strong>局部变量表</strong>：存放了编译期可知的各种基本数据类型(64位的long和double占据2个局部变量空间Slot)、对象引用（指向对象起始地址的引用指针，或者指向一个代表对象的句柄，或者其他与此对象相关的位置），和<strong>returnAddress</strong>类型（指向了一条<strong>字节码指令</strong>的地址）</li>
<li>局部变量表所需要的内存在编译期间完成分配，运行期间不会改变局部变量表的大小。</li>
<li>两种异常：<ul>
<li>线程请求栈的深度大于虚拟机允许：StackOverflowError</li>
<li>如果虚拟机栈可以动态扩展，如果扩展式无法申请到足够的内存：OutOfMemoryError</li>
</ul>
</li>
</ul>
<h4 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h4><ul>
<li>与虚拟机栈类似，虚拟机栈为虚拟机执行Java方法（也就是<strong>字节码</strong>），本地方法栈为执行Native方法服务。</li>
<li><a href="http://blog.csdn.net/wike163/article/details/6635321" target="_blank" rel="external">Native方法</a>：一个Native Method就是一个java调用非java代码的接口。一个Native Method是这样一个java的方法：该方法的实现由非java语言实现，比如C。</li>
</ul>
<h4 id="Java堆"><a href="#Java堆" class="headerlink" title="Java堆"></a>Java堆</h4><ul>
<li>是Java虚拟机所管理的内存中最大的一块，被所有线程共享的一块内存区域，在虚拟机启动时创建。</li>
<li>Java堆的唯一目的：存放对象实例。</li>
<li>随着<strong>JIT编译器</strong>的发展和<strong>逃逸分析技术</strong>逐渐成熟，所有对象都都分配在堆上，已经不那么绝对了。</li>
<li>Java堆是垃圾收集器管理的主要区域，因此也被称作GC堆。（Garbage Collected）</li>
<li>从内存回收的角度，由于基本采用<strong>分代收集算法</strong>：<ul>
<li>Java堆可分为：<ul>
<li>新生代</li>
<li>老年代</li>
</ul>
</li>
<li>更细致一点：<ul>
<li>Eden空间</li>
<li>From Survivor空间</li>
<li>To Survivor空间</li>
</ul>
</li>
</ul>
</li>
<li>从内存分配的角度，可能划分出多个线程私有的分配缓冲区（TLAB）（Tread Local Allocation Buffer）</li>
<li>Java可以处于物理上不连续的内存空间中，只要逻辑上连续的即可。</li>
<li>可以固定大小，也可以扩展（<strong>通过-Xmx和-Xms控制</strong>）</li>
</ul>
<h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><ul>
<li>用于存储已被虚拟机加载的<strong>类信息</strong>、<strong>常量</strong>、<strong>静态变量</strong>、<strong>即时编译器编译后的代码</strong>等。</li>
<li>这区域的内存回收目标主要是针对<strong>常量池的回收</strong>和<strong>对类型的卸载</strong>，也可以不实现垃圾收集。</li>
</ul>
<h5 id="运行时的常量池"><a href="#运行时的常量池" class="headerlink" title="运行时的常量池"></a>运行时的常量池</h5><ul>
<li>Class文件中的常量池（Constant Pool Table），用于存放编译期生成的各种<strong>字面量</strong>和<strong>符号引用</strong>。这些内容在类加载后进入方法区的运行时常量池。</li>
<li>除了保存Class文件中描述的<strong>符号引用</strong>外，还会把翻译出来的<strong>直接引用</strong>也存储在运行时常量池。</li>
<li>不一定编译的时候产生常量，运行期间也可能加入新的常量，比如说String.intern()<ul>
<li>intern()：当调用 intern 方法时，如果池已经包含一个等于此 String 对象的字符串（该对象由 equals(Object) 方法确定），则返回池中的字符串。否则，将此 String 对象添加到池中，并且返回此 String 对象的引用。</li>
</ul>
</li>
</ul>
<h4 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h4><ul>
<li>JDK1.4新加入NIO类，引入基于<strong>通道(Channel)</strong>与<strong>缓冲区(Buffer)</strong>的I/O方式，可以使用Native函数库直接分配堆外内存，然后通过Java堆中的DirectByteBuffer对象作为对这块内存的引用。避免了再Java堆和Native堆中来回复制数据。</li>
<li>不能忽视直接内存，以免出现OutOfMemoryError异常。</li>
</ul>
<h3 id="HotSpot虚拟机对象探秘"><a href="#HotSpot虚拟机对象探秘" class="headerlink" title="HotSpot虚拟机对象探秘"></a>HotSpot虚拟机对象探秘</h3><h4 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h4><ul>
<li>虚拟机遇到一条new指令，检查这个指令的参数是否能在常量池中定位到一个类的符号引用。并且检查这个符号引用代表的类是否被<strong>加载、解析、初始化过</strong>。如果没有，先执行相应的类加载过程。（可以确定类需要的内存大小）</li>
<li>虚拟机为新生对象分配内存。<ul>
<li>如果Java堆中内存是绝对规整的，把作为分界点指示器的指针，向前挪动一段与对象大小相等的距离。——叫做指针碰撞（Bump the Pointer）。</li>
<li>如果Java堆中内存不是规整的，虚拟机需要维护一个列表，记录那些内存块是可用的。——空闲列表（Free List）。</li>
<li>Java堆是否规整由所采用的垃圾收集器是否带有压缩整理功能决定的。</li>
<li>保证修改指针的线程安全<ol>
<li>采用<strong>CAS</strong>配上<strong>失败重试</strong>的方式保证更新操作的原子性。</li>
<li>另一种方法是把内存分配的动作按照线程划分在不同的空间进行。每个线程在Java堆中预先分配一小块内存，成为本地线程分配缓存（TLAB）</li>
</ol>
</li>
</ul>
</li>
<li>内存分配完成后，虚拟机将分配到的内存空间都初始化为零值（<strong>不包括对象头</strong>）。保证对象的实例字段在Java代码中可以不赋初始值就直接使用。</li>
<li>对对象进行必要的设置<ul>
<li>这个对象是哪个类的实例</li>
<li>如何才能找到类的元数据信息</li>
<li>对象的哈希码</li>
<li>对象的<strong>GC分代年龄</strong></li>
</ul>
</li>
<li>最后执行ini()方法。</li>
</ul>
<h4 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h4><ul>
<li>对象头（Header）<ul>
<li>用于存储对象自身的运行时数据。如哈希码，GC分代年龄，锁状态标志，线程持有的锁，偏向想成ID，偏向时间戳等。官方称为Mark Word。</li>
<li>类型指针。即对象指向它的<strong>类元数据</strong>的指针。通过这个指针来确定这个对象是哪个类的实例。</li>
</ul>
</li>
<li>实例数据（Instance Data）<ul>
<li>在程序代码中所定义的各种类型的字段内容。</li>
<li>存储顺序收到虚拟机分配策略参数（FieldsAllocationStyle）和字段在Java源码中定义顺序的影响。<ul>
<li>相同宽度的字段被分配在一起</li>
<li>父类中定义的变量会出现在自雷之前。</li>
</ul>
</li>
</ul>
</li>
<li>对齐填充（Padding）<ul>
<li>对象的大小必须是8字节的整数倍。</li>
</ul>
</li>
</ul>
<h4 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h4><ul>
<li>Java程序需要通过栈上的<strong>reference数据</strong>来操作堆上的具体对象。</li>
<li>使用句柄的话，Java堆中会划分出一块内存来作为句柄池，reference中存储的就是对象的句柄地址。据病种包含了对象<strong>实例数据</strong>与<strong>类型数据</strong>各自的具体地址信息。<ul>
<li>优点：对象被移动时，只要改变据病种的实例数据指针，reference本身不需要改变。</li>
</ul>
</li>
<li>使用直接指针访问的话，Java堆对象的布局中就必须考虑放置访问类型数据相关信息。<ul>
<li>好处：速度更快，节省了一次指针定位的时间开销。</li>
</ul>
</li>
</ul>
<h3 id="OOM异常实战"><a href="#OOM异常实战" class="headerlink" title="OOM异常实战"></a>OOM异常实战</h3><h4 id="Java堆溢出"><a href="#Java堆溢出" class="headerlink" title="Java堆溢出"></a>Java堆溢出</h4><ul>
<li>通过内存映像分析工具对Dump出来的<strong>堆转储快照</strong>进行分析，确认内存中的对象是否是必要的，也就是分清楚是内存泄漏(Memory Leak)还是内存溢出（Memory Overflow）。</li>
<li>内存泄漏（GC无法回收它们）<ul>
<li>通过工具查看泄漏对象是通过怎样的路径与<strong>GC Roots</strong>相关联，并导致垃圾回收器，无法自动回收它们的。</li>
</ul>
</li>
<li>内存溢出（对象都必须存活）<ul>
<li>检查堆参数（Xms和Xmx）与物理内存的比例看是否还可以增大。</li>
<li>从代码上检查是否存在某些对象<strong>生命周期过长</strong>，<strong>持有状态时间过长</strong>。</li>
</ul>
</li>
</ul>
<h4 id="虚拟机栈和本地方法栈溢出"><a href="#虚拟机栈和本地方法栈溢出" class="headerlink" title="虚拟机栈和本地方法栈溢出"></a>虚拟机栈和本地方法栈溢出</h4><ul>
<li>如果线程请求的栈深度大于虚拟机所允许的最大深度，StackOverflowError.</li>
<li>如果虚拟机在扩展式无法申请到足够的内存空间，OutMemoryError.</li>
<li>在单个线程下，虚拟机抛出都是StackOverflowError。</li>
</ul>
<h4 id="方法区和运行时常量池溢出"><a href="#方法区和运行时常量池溢出" class="headerlink" title="方法区和运行时常量池溢出"></a>方法区和运行时常量池溢出</h4><ul>
<li>String.intern()</li>
</ul>
<h4 id="本机内存直接溢出"><a href="#本机内存直接溢出" class="headerlink" title="本机内存直接溢出"></a>本机内存直接溢出</h4><ul>
<li>特征是在Heap Dump文件中不会看见明显的异常，如果发现OOM之后Dump文件很小，程序又使用了NIO，可以考虑一下这个原因。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/08/TensorFlow入门——MNIST手写字符识别/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/08/TensorFlow入门——MNIST手写字符识别/" itemprop="url">
                  TensorFlow入门——MNIST手写字符识别
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-08T22:01:55+08:00">
                2017-04-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/08/TensorFlow入门——MNIST手写字符识别/" class="leancloud_visitors" data-flag-title="TensorFlow入门——MNIST手写字符识别">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>内容翻译自<a href="https://www.tensorflow.org/get_started/mnist/pros" target="_blank" rel="external">Deep MNIST for Experts</a></p>
<p>本文可能插入TensorFlow官网上的图片或者链接，需翻墙可以加载或访问。</p>
<p>TenforFlow是一个强大的大规模数字计算的软件包。它擅长的功能之一是实现并且训练深度神经网络。在这个教程中，我们将会学习构建一个基本的深度卷积MNIST分类模型，</p>
<h2 id="关于这个教程"><a href="#关于这个教程" class="headerlink" title="关于这个教程"></a>关于这个教程</h2><p>这个教程的第一部分解释在<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="external">mnist_softmax.py</a>代码中实现了什么，这是TensorFlow模型实现的基础。第二部分展示一些提高精确度的方法。</p>
<p>在这个教程中我们将会完成以下几点：</p>
<ul>
<li>建立一个softmax回归函数作为识别MNIST数字的模型，基于观察图片中的每一个像素。</li>
<li>使用TensorFlow训练这个模型去识别图片，通过观察成千的样本。</li>
<li>使用测试数据验证模型的精确度。</li>
<li>建立、训练、测试一个多层卷积神经网络，来提升结果。</li>
</ul>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>开始之前，要先加载MNIST数据集，然后开始一个TensorFlow session</p>
<h3 id="加载MNIST数据"><a href="#加载MNIST数据" class="headerlink" title="加载MNIST数据"></a>加载MNIST数据</h3><p>复制粘贴下面的代码，程序会自动下载读取数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这里<code>mnist</code>是一个轻量的类储存训练、验证、测试数据，以NumPy数组的方式。其也提供了一个迭代小批的函数，我们下面会用到。</p>
<h3 id="开始TensorFlow-InterativeSession"><a href="#开始TensorFlow-InterativeSession" class="headerlink" title="开始TensorFlow InterativeSession"></a>开始TensorFlow InterativeSession</h3><p>TensorFlow在后端依靠高效的C++来执行计算。与后端的连接叫做<code>sessioin</code>。TensorFlow程序常见的用法是创建一个<code>graph</code>，然后再一个<code>session</code>中启动它。</p>
<p>这里我们选择使用方便的<code>InteractiveSession</code>类，它会让TensorFlow中你组织你的代码更灵活。它循序你交替运行建立一个计算图和运行这个图的操作。（ It allows you tointerleave operations which build a <a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">computation graph</a> with ones that run the graph. ）这在例如IPython的交互式的环境中特别方便。如果你不使用<code>InteractiveSesion</code>，你应该建立一个完整的图在你启动session和运行这个图之前。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">sess = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>为了在Python中高效的进行数值计算，我们使用像NumPy的包来在Python以外进行重量级运算，例如矩阵乘法。不幸的是，每个操作仍然有很多切换回Python的开销。如果你想运行这些计算在GPU上或者以分布式的形式这些在转换数据时有大量消耗的方式，这些开销变的相当昂贵。</p>
<p>TensorFlow也是把这些重量操作放在Python外面，但是它做了一些设定去避免这些开销。TensorFlow允许我们描述一个图在Python外面运行的交互操作，而不是使用Python运行一个单独的重量操作。这个方法和在Theano和Torch中的方法很像。</p>
<p>Python代码的作用是建立一个外部计算图，然后命令运算图的哪一部分应该被执行。查看<a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">Computation Graph</a>和<a href="https://www.tensorflow.org/get_started/get_started" target="_blank" rel="external">Getting Started With TensorFlow</a>获得更多细节。</p>
<h2 id="建立一个Softmax回归模型"><a href="#建立一个Softmax回归模型" class="headerlink" title="建立一个Softmax回归模型"></a>建立一个Softmax回归模型</h2><p>在这一部分，我们建立一个单层softmax回归模型，下一节我们将模型扩展为多层卷积网络。</p>
<h3 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h3><p>我们通过创建输入图像的节点和输出类别的节点来建立计算图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>输入的图像x由二维浮点数<code>tensor</code>组成。我们分配一个<code>[None,784]</code> 的<code>shape</code>。784是28*28图像，None代表第一位由每一批的数量决定。目标输出类别<code>y_</code>由二维<code>tensor</code>组成，每行是一个十维的矢量表示类别（0或者1）。</p>
<p><code>placeholder</code> 的<code>shape</code>参数是可选择的，但是它允许TensorFlow在<code>shape</code>不一致的时候自动的捕获bug。</p>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>我们定义权重<code>W</code>和偏置量<code>b</code>，我们可以设想通过额外的输入训练这些量。但是TensorFlow有一种更好的方式管理他们：<code>variable</code>。一个<code>varable</code>是TensorFlow的计算图中的一个值。他可以根据计算修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure>
<p>我们在调用<code>tf.Variable</code>的时候传入一个初始值。在这个例子中，我们将w和b都初始为0.</p>
<p>在<code>Varialbe</code>在session中调用之前，他们必须使用这个session初始化。这一步让已经指定的初始值赋值给每个变量。可以为所有变量一次性全部初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sess.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure>
<h3 id="预测类别以及损失函数"><a href="#预测类别以及损失函数" class="headerlink" title="预测类别以及损失函数"></a>预测类别以及损失函数</h3><p>我们现在可以实现我们的回归模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y = tf.matmul(x,W) + b</div></pre></td></tr></table></figure>
<p>我们可以和这一样简单的指定损失函数。这里，我们的损失函数是目标和这个预测模型softmax激活函数的交叉熵。作为初级教程，我们使用这个保险的形式:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cross_entropy = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</div></pre></td></tr></table></figure>
<p>注意这里<code>tf.nn.softmax_corss_entropy_with_logits</code>internally(内部地) applies the softmax on the model’s unnormalized model prediction and sums across all classes。<code>tf.reduce_mean</code>求这些和的平均数。</p>
<h2 id="训练这个模型"><a href="#训练这个模型" class="headerlink" title="训练这个模型"></a>训练这个模型</h2><p>现在我们已经定义了我们的模型和损失函数，这让使用TensorFlow训练很简单。因为TensorFlow知道了所有的计算图，他可以使用自动的计算损失函数对于每一个变量的导数。TensorFlow有很多不同的<a href="https://www.tensorflow.org/api_guides/python/train#optimizers" target="_blank" rel="external">自带最优化算法</a>。例如我们使用梯度下降法，步长0.5去减少交叉熵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>这里在计算图上增加了一个新的操作，这个操作包含计算梯度、计算参数更新量、然后更新参数。</p>
<p>运行时被返回的操作<code>train_step</code>会应用到梯度下降中更新参数。因此，训练这个模型可以不停的运行<code>train_step</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</div></pre></td></tr></table></figure>
<p>我们在每次训练迭代的时候加载100条训练样本。当我们运行<code>train_step</code>操作，使用<code>feed_dict</code>去取代<code>placeholder</code>tensors<code>x</code>和<code>y_</code>。注意，你可以使用<code>feed_dict</code>替代任何tensor在你的计算图中，不仅仅是<code>placeholder</code>.</p>
<h3 id="评价这个模型"><a href="#评价这个模型" class="headerlink" title="评价这个模型"></a>评价这个模型</h3><p>首先我们计算出我们预测正确标签的位置。<code>tf.argmax</code>是一个特别有用的函数，可以给你tensor中最高标量的索引。例如<code>tf.argmax(y,1)</code>是我们模型认为最有可能的结果，当<code>tf.argmax(y_,1)</code>是一个正确的标签。我们使用<code>tf.equal</code>去检查我们的预测是否正确：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>返回我们一个布尔值。为了确定正确率，我们转换为浮点数，然后计算他们的平均数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<p>最终我们评价在测试数据上的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure>
<h2 id="构建一个多层卷积网络"><a href="#构建一个多层卷积网络" class="headerlink" title="构建一个多层卷积网络"></a>构建一个多层卷积网络</h2><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>为了建立这个模型，我们将需要很多权重和偏置量。应该用一个很小的量初始化这些权重来打破对称，和预防0梯度。所以，我们使用<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)神经细胞。用一些小的正值初始化偏置量可以避免”死神经细胞“。为了防止在建立模型时重复这一过程，我们建立两个方便的函数去做这些事情：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<h3 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h3><p>TensorFlow也给予我们在在卷积和池化中很多灵活性。我们要怎样处理边界？步长多少？在这个例子中，我们将选择vanilla版本。我们的卷积使用步长1，pad 0.所以，输出和输入的规格是一样的。我们2*2格子中选择最大池化。为了保持代码整洁，仍然建立两个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<p><strong>padding</strong> :步长不能整除图片大小时边距的处理方法，“SAME”表示超过边界用0填充，使得输出保持和输入相同的大小，“VALID”表示不超过边界，通常会丢失一些信息。</p>
<h3 id="第一个卷积层"><a href="#第一个卷积层" class="headerlink" title="第一个卷积层"></a>第一个卷积层</h3><p>我们可以实现我们的第一层。卷积将会计算5*5patch的32个特征。权重tensor的shape是[5,5,1,32]。前两个维度是patch大小。下一个数字是输入频道数，最后一个是输出频道数。而且我们将为每个输出加上一个偏置量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>我们先将<code>x</code>改造成4维的tensor。第二第三维对应图像的宽和长，最后一维对应彩色通道（channel）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div></pre></td></tr></table></figure>
<p>使用权重卷积，加上偏置量，应用ReLU函数，最后最大池化。池化将会把图片大小改变为14*14.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure>
<h3 id="第二个卷积层"><a href="#第二个卷积层" class="headerlink" title="第二个卷积层"></a>第二个卷积层</h3><p>第二层有对于每个5*5的patch，有64个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>为了减少过拟合，我们在输出之前应用<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">dropout</a>。我们创建一个神经元输出概率的<code>placeholder</code>。在训练的时候弃用，在测试的时候关闭。TensorFlow的<code>tf.nn.dropout</code>自动的控制神经输出时mask他们。所以dropout不需要额外的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line">b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line"></div><div class="line">y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</div></pre></td></tr></table></figure>
<h3 id="训练并且评价这个模型"><a href="#训练并且评价这个模型" class="headerlink" title="训练并且评价这个模型"></a>训练并且评价这个模型</h3><p>和softmax神经网络的区别：</p>
<ul>
<li>使用复杂的ADAM optimizer取代梯度下降法。</li>
<li>在<code>feed_dict</code> 中使用<code>keep_prob</code>控制dropout率。</li>
<li>训练过程中x每100次迭代输出一次日志。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">sess.run(tf.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</div><div class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line">    print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</div><div class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</div><div class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/07/TensorFlow入门——线性回归的例子/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/07/TensorFlow入门——线性回归的例子/" itemprop="url">
                  TensorFlow入门——线性回归的例子
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-07T18:31:37+08:00">
                2017-04-07
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          
             <span id="/2017/04/07/TensorFlow入门——线性回归的例子/" class="leancloud_visitors" data-flag-title="TensorFlow入门——线性回归的例子">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>翻译自<a href="https://www.tensorflow.org/get_started/get_started" target="_blank" rel="external">Getting Started With TensorFlow</a> </p>
<p>TensorFlow提供很多API，低级API<code>TensorFlow Core</code>提供完整的程序控制。向机器学习研究者和 其他想对自己的模型有良好控制的人推荐<code>TensorFlow Core</code>。高级API是在<code>Tensorflow Core</code>的上层上建立的。高级API比<code>TensorFlow Core</code>学习和使用都要简单。而且，高级API让重复性的任务更简单，在不同的用户手中更一致。高级API像<code>tf.contrib.learn</code>帮助你管理数据集，估计量（estimators），训练和推理（inference）。注意，一些高级API名字中带有<code>contrib</code>的仍在开发中，<code>contrib</code>方法可能会在后来的版本中弃用。</p>
<h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>TensorFlow里的核心数据元素是<code>tesnor</code>。一个<code>tensor</code>包含一个任何维度数组形式的原值的集合。<code>tensor</code>的<code>rank</code>是维度的数目。下面上一些例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">3</span> <span class="comment"># a rank 0 tensor; this is a (scalar)标量 with shape []</span></div><div class="line">[<span class="number">1.</span> ,<span class="number">2.</span>, <span class="number">3.</span>] <span class="comment"># a rank 1 tensor; this is a vector with shape [3]</span></div><div class="line">[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]] <span class="comment"># a rank 2 tensor; a matrix with shape [2, 3]</span></div><div class="line">[[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], [[<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]]] <span class="comment"># a rank 3 tensor with shape [2, 1, 3]</span></div></pre></td></tr></table></figure>
<h2 id="TensorFlow-Core-教程"><a href="#TensorFlow-Core-教程" class="headerlink" title="TensorFlow Core 教程"></a>TensorFlow Core 教程</h2><h3 id="Importing-TensorFlow"><a href="#Importing-TensorFlow" class="headerlink" title="Importing TensorFlow"></a>Importing TensorFlow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>导入TensorFlow的所有classes，methods，symbols</p>
<h3 id="The-Computational-Graph"><a href="#The-Computational-Graph" class="headerlink" title="The Computational Graph"></a>The Computational Graph</h3><p>你可以把TensorFlow Core程序看成是两段的集合：</p>
<ol>
<li>建立计算图</li>
<li>运行计算图</li>
</ol>
<p>计算图（Computational Graph）是排列成图节点的一系列操作。每个节点将零个或多个tensor作为输入，产生一个tensor作为输出。constant是其中一种节点，它没有输入，输出一个内部储存的值。创建两个浮点数Tensors，<code>node1</code>和<code>node2</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node1 = tf.constant(<span class="number">3.0</span>, tf.float32)</div><div class="line">node2 = tf.constant(<span class="number">4.0</span>) <span class="comment"># also tf.float32 implicitly</span></div><div class="line">print(node1, node2)</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=float32) Tensor(<span class="string">"Const_1:0"</span>, shape=(), dtype=float32)</div></pre></td></tr></table></figure>
<p>并没有输出3.0和4.0。因为他们是节点（node），当我们evaluate时，会分别产生3.0和4.0。要想evaluate这些节点，我们必须使用session运行计算图。一个session内部封装了TensorFlow运行时刻的控制和状态。</p>
<p>下面的代码创建了一个<code>Session</code>对象，我们调用他的<code>run</code>方法去运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">print(sess.run([node1, node2]))</div></pre></td></tr></table></figure>
<p>得到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">3.0</span>,<span class="number">4.0</span>]</div></pre></td></tr></table></figure>
<p><strong>我运行还会出现很多错误：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; node1 = tf.constant(3.0,tf.float32)</div><div class="line">&gt;&gt;&gt; sess = tf.Session()</div><div class="line">&gt;&gt;&gt; print(sess.run(node1))</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;BestSplits&quot; device_type: &quot;CPU&quot;&apos;) fo</div><div class="line">r unknown op: BestSplits</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;CountExtremelyRandomStats&quot; device_t</div><div class="line">ype: &quot;CPU&quot;&apos;) for unknown op: CountExtremelyRandomStats</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;FinishedNodes&quot; device_type: &quot;CPU&quot;&apos;)</div><div class="line"> for unknown op: FinishedNodes</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;GrowTree&quot; device_type: &quot;CPU&quot;&apos;) for</div><div class="line">unknown op: GrowTree</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;ReinterpretStringToFloat&quot; device_ty</div><div class="line">pe: &quot;CPU&quot;&apos;) for unknown op: ReinterpretStringToFloat</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;SampleInputs&quot; device_type: &quot;CPU&quot;&apos;)</div><div class="line">for unknown op: SampleInputs</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;ScatterAddNdim&quot; device_type: &quot;CPU&quot;&apos;</div><div class="line">) for unknown op: ScatterAddNdim</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;TopNInsert&quot; device_type: &quot;CPU&quot;&apos;) fo</div><div class="line">r unknown op: TopNInsert</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;TopNRemove&quot; device_type: &quot;CPU&quot;&apos;) fo</div><div class="line">r unknown op: TopNRemove</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;TreePredictions&quot; device_type: &quot;CPU&quot;</div><div class="line">&apos;) for unknown op: TreePredictions</div><div class="line">E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core</div><div class="line">\framework\op_kernel.cc:943] OpKernel (&apos;op: &quot;UpdateFertileSlots&quot; device_type: &quot;C</div><div class="line">PU&quot;&apos;) for unknown op: UpdateFertileSlots</div><div class="line">3.0</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>待搞明白。</p>
<p>我们可以建立更复杂的计算通过组合<code>Tensor</code>节点和操作（操作也是节点）。例如，我们可以把我们的两个常量节点相加，产生一个新的图（Graph）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node3 = tf.add(node1, node2)</div><div class="line">print(<span class="string">"node3: "</span>, node3)</div><div class="line">print(<span class="string">"sess.run(node3): "</span>,sess.run(node3))</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node3:  Tensor(<span class="string">"Add_2:0"</span>, shape=(), dtype=float32)</div><div class="line">sess.run(node3):  <span class="number">7.0</span></div></pre></td></tr></table></figure>
<p>TensorFlow 提供一个使用的TensorBoard可以展示计算图的图片。<img src="https://www.tensorflow.org/images/getting_started_add.png" alt="TensorBoard screenshot"></p>
<p>目前，图片没有特别英吹思婷，因为它经常产生恒定的结果。图可以使用参数表示外部的输入，就像<code>placeholders</code>。一个<code>placeholder</code>是一个后面会提供值的promise。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = tf.placeholder(tf.float32)</div><div class="line">b = tf.placeholder(tf.float32)</div><div class="line">adder_node = a + b  <span class="comment"># + provides a shortcut for tf.add(a, b)</span></div></pre></td></tr></table></figure>
<p>我们可以使用多个输入evaluate这个graph，通过使用feed_dict参数为这些placeholders提供常量去specifty Tensors。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(sess.run(adder_node, &#123;a: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line">print(sess.run(adder_node, &#123;a: [<span class="number">1</span>,<span class="number">3</span>], b: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">7.5</span></div><div class="line">[ <span class="number">3.</span>  <span class="number">7.</span>]</div></pre></td></tr></table></figure>
<p>在机器学习中，我们希望模型可以有任意输入。为了让模型可以训练，我们需要修改图去从相同的输入得到新的输出。<code>Variables</code>允许我们为图添加可训练的参数。他们由初始值和类型构成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable([<span class="number">.3</span>], tf.float32)</div><div class="line">b = tf.Variable([<span class="number">-.3</span>], tf.float32)</div><div class="line">x = tf.placeholder(tf.float32)</div><div class="line">linear_model = W * x + b</div></pre></td></tr></table></figure>
<p><code>variables</code>不会在你使用<code>tf.Variable</code>的时候初始化。你必须明确的使用下面的操作去初始化所有的变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure>
<p><code>init</code>是TensorFlow子图中初始化所有全局变量的handle，一直到我们调用<code>sess.run</code>之前，这些变量都是未初始化的状态。</p>
<p>我们可以同时使用多个X的值计算(evaluate) <code>linear_model</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(sess.run(linear_model, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[ <span class="number">0.</span>          <span class="number">0.30000001</span>  <span class="number">0.60000002</span>  <span class="number">0.90000004</span>]</div></pre></td></tr></table></figure>
<p>我们使用标准的线性回归损失模型：计算当前预测值和实际值的平方和。<code>linear_model - y</code>。我们调用<code>tf.quare</code>，然后使用<code>tf.reduce_sum</code>相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">y = tf.placeholder(tf.float32)</div><div class="line">squared_deltas = tf.square(linear_model - y)</div><div class="line">loss = tf.reduce_sum(squared_deltas)</div><div class="line">print(sess.run(loss, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;))</div></pre></td></tr></table></figure>
<p>计算出损失值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">23.66</span></div></pre></td></tr></table></figure>
<p>变量以<code>tf.Variable</code>的值初始化，可以通过<code>tf.assign</code>操作改变。比如<code>W=-1,b=1</code>是模型的最佳参数，我们可以通过以下改变W和b的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fixW = tf.assign(W, [<span class="number">-1.</span>])</div><div class="line">fixb = tf.assign(b, [<span class="number">1.</span>])</div><div class="line">sess.run([fixW, fixb])</div><div class="line">print(sess.run(loss, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.0</span></div></pre></td></tr></table></figure>
<p>下面展示如何自动的找到模型的最佳参数。</p>
<h2 id="tf-train-API"><a href="#tf-train-API" class="headerlink" title="tf.train API"></a>tf.train API</h2><p>TensorFlow提供<code>optimizers</code>可以缓慢的改变变量，为了最小化损失函数。最简单的最优化算法是梯度下降法。它根据参数对于损失函数导数的大小改变每个参数。通常，计算符号求导很容易出错，而且没意思。所以，TensorFlow可以使用函数<code>tf.gradients</code>自动的求的导数，只要根据模型的描述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line">train = optimizer.minimize(loss)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sess.run(init) <span class="comment"># reset values to incorrect defaults.</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  sess.run(train, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;)</div><div class="line"></div><div class="line">print(sess.run([W, b]))</div></pre></td></tr></table></figure>
<p>最终的模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[array([<span class="number">-0.9999969</span>], dtype=float32), array([ <span class="number">0.99999082</span>],</div><div class="line"> dtype=float32)]</div></pre></td></tr></table></figure>
<p>TensorFlow提供常见模式、结构、功能的更高级抽象。下一节学习如何使用这些。</p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Model parameters</span></div><div class="line">W = tf.Variable([<span class="number">.3</span>], tf.float32)</div><div class="line">b = tf.Variable([<span class="number">-.3</span>], tf.float32)</div><div class="line"><span class="comment"># Model input and output</span></div><div class="line">x = tf.placeholder(tf.float32)</div><div class="line">linear_model = W * x + b</div><div class="line">y = tf.placeholder(tf.float32)</div><div class="line"><span class="comment"># loss</span></div><div class="line">loss = tf.reduce_sum(tf.square(linear_model - y)) <span class="comment"># sum of the squares</span></div><div class="line"><span class="comment"># optimizer</span></div><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line">train = optimizer.minimize(loss)</div><div class="line"><span class="comment"># training data</span></div><div class="line">x_train = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line">y_train = [<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]</div><div class="line"><span class="comment"># training loop</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init) <span class="comment"># reset values to wrong</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  sess.run(train, &#123;x:x_train, y:y_train&#125;)</div><div class="line"></div><div class="line"><span class="comment"># evaluate training accuracy</span></div><div class="line">curr_W, curr_b, curr_loss  = sess.run([W, b, loss], &#123;x:x_train, y:y_train&#125;)</div><div class="line">print(<span class="string">"W: %s b: %s loss: %s"</span>%(curr_W, curr_b, curr_loss))</div></pre></td></tr></table></figure>
<h2 id="tf-contrib-learn"><a href="#tf-contrib-learn" class="headerlink" title="tf.contrib.learn"></a>tf.contrib.learn</h2><p><code>tf.contrib.learn</code>是高级TensorFlow包，简化了机器学习的机制，包括：</p>
<ul>
<li>运行训练循环</li>
<li>运行评价循环</li>
<li>管理数据集</li>
<li>管理输入（feeding？）</li>
</ul>
<p>tf.contrib.learn定义了很多的常见模型</p>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p>使用<code>tf.contrib.learn</code>简化线性回归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># NumPy 常常被用来加载、操作、预处理数据</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 声明特征List. 我们只有一个实值（real-valued）特征. There are many other types of columns that are more complicated and useful.</span></div><div class="line">features = [tf.contrib.layers.real_valued_column(<span class="string">"x"</span>, dimension=<span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># estimator在前端调用训练（fitting）和评价（ingerence）.有很多事先定义好的类型，比如说线性回归，罗杰斯特回归，线性分类，罗杰斯特分类，和很多神经网络分类和回归. 下面的代码提供一个线性回归estimator。</span></div><div class="line">estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)</div><div class="line"></div><div class="line"><span class="comment"># Tensorflow提供了很多有用的方法读取和建立数据集. 这里我们使用`numpy_input_fn`.我们需要告诉函数，我们需要多少数据batches（num_epochs）和每个batch多大.</span></div><div class="line">x = np.array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</div><div class="line">y = np.array([<span class="number">0.</span>, <span class="number">-1.</span>, <span class="number">-2.</span>, <span class="number">-3.</span>])</div><div class="line"><span class="comment">#这里的num_epochs，和下面的steps是不是要一致？</span></div><div class="line">input_fn = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>:x&#125;, y, batch_size=<span class="number">4</span>,</div><div class="line">                                              num_epochs=<span class="number">1000</span>)</div><div class="line"></div><div class="line"><span class="comment">#我们可以调用1000次训练步骤通过使用'fit'方法，然后输入训练数据集.</span></div><div class="line">estimator.fit(input_fn=input_fn, steps=<span class="number">1000</span>)</div><div class="line"></div><div class="line"><span class="comment">#这里我们评价我们模型有多么合适，在一个实际例子中，我们使用分别评价(separate validation)和测试数据集去避免过拟合.</span></div><div class="line">estimator.evaluate(input_fn=input_fn)</div></pre></td></tr></table></figure>
<p>疑惑：separate validation?  num_epochs?</p>
<h3 id="一个自定义模型"><a href="#一个自定义模型" class="headerlink" title="一个自定义模型"></a>一个自定义模型</h3><p><code>tf.contrib.learn</code>不会把你限制在这些预先定义的模型当中。假设我们想创建一个TensorFlow没有实现的自定义模型，我们仍然可以保持<code>tf.contrin.learn</code>对于数据集、输入(feeding)、训练的高度抽象。我们下面通过使用低级TensorFlowAPI来实现线性回归。</p>
<p>使用<code>tf.contrib.learn</code>自定义一个模型，我们需要使用<code>tf.contrib.learn.estimator</code>。<code>tf.contrib.learn.LinearRegressor</code>实际上是<code>tf.contrib.learn.Estimator</code>的一个子集。我们向提供<code>Estimator</code>一个函数 <code>model_fn</code>来告诉<code>tf.contrib.learn</code>要怎么样评价、预测、训练步数、损失函数。</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># Declare list of features, we only have one real-valued feature</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(features, labels, mode)</span>:</span></div><div class="line">  <span class="comment"># Build a linear model and predict values</span></div><div class="line">  W = tf.get_variable(<span class="string">"W"</span>, [<span class="number">1</span>], dtype=tf.float64)</div><div class="line">  b = tf.get_variable(<span class="string">"b"</span>, [<span class="number">1</span>], dtype=tf.float64)</div><div class="line">  y = W*features[<span class="string">'x'</span>] + b</div><div class="line">  <span class="comment"># Loss sub-graph</span></div><div class="line">  loss = tf.reduce_sum(tf.square(y - labels))</div><div class="line">  <span class="comment"># Training sub-graph</span></div><div class="line">  global_step = tf.train.get_global_step()</div><div class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line">  train = tf.group(optimizer.minimize(loss),</div><div class="line">                   tf.assign_add(global_step, <span class="number">1</span>))</div><div class="line">  <span class="comment"># ModelFnOps connects subgraphs we built to the</span></div><div class="line">  <span class="comment"># appropriate functionality.</span></div><div class="line">  <span class="keyword">return</span> tf.contrib.learn.ModelFnOps(</div><div class="line">      mode=mode, predictions=y,</div><div class="line">      loss=loss,</div><div class="line">      train_op=train)</div><div class="line"></div><div class="line">estimator = tf.contrib.learn.Estimator(model_fn=model)</div><div class="line"><span class="comment"># define our data set</span></div><div class="line">x = np.array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</div><div class="line">y = np.array([<span class="number">0.</span>, <span class="number">-1.</span>, <span class="number">-2.</span>, <span class="number">-3.</span>])</div><div class="line">input_fn = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>: x&#125;, y, <span class="number">4</span>, num_epochs=<span class="number">1000</span>)</div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">estimator.fit(input_fn=input_fn, steps=<span class="number">1000</span>)</div><div class="line"><span class="comment"># evaluate our model</span></div><div class="line">print(estimator.evaluate(input_fn=input_fn, steps=<span class="number">10</span>))</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://learning.com/2017/04/06/LeetCode-一种递归除方法/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhuo Jimmy">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="jlearning.cn">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="jlearning.cn" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/06/LeetCode-一种递归除方法/" itemprop="url">
                  LeetCode_一种递归除方法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-06T23:15:21+08:00">
                2017-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法笔记/" itemprop="url" rel="index">
                    <span itemprop="name">算法笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          
             <span id="/2017/04/06/LeetCode-一种递归除方法/" class="leancloud_visitors" data-flag-title="LeetCode_一种递归除方法">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>LeetCode 29 不适用乘除求余运算，设计一个除法算法。</p>
<p>我的答案:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (ldivisor == <span class="number">0</span>) <span class="keyword">return</span> Integer.MAX_VALUE;</div><div class="line">	    <span class="keyword">if</span> ((ldividend == <span class="number">0</span>) || (ldividend &lt; ldivisor))	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">	    </div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">		<span class="keyword">if</span>(dividend&gt;=<span class="number">0</span>)</div><div class="line">			<span class="keyword">if</span>(divisor&gt;<span class="number">0</span>)</div><div class="line">				<span class="keyword">while</span>(dividend-divisor&gt;=<span class="number">0</span>)&#123;</div><div class="line">					dividend = dividend-divisor;</div><div class="line">					res++;</div><div class="line">				&#125;</div><div class="line">			<span class="keyword">else</span></div><div class="line">				<span class="keyword">while</span>(dividend+divisor&gt;=<span class="number">0</span>)&#123;</div><div class="line">					dividend = dividend+divisor;</div><div class="line">					res--;</div><div class="line">				&#125;</div><div class="line">		<span class="keyword">else</span></div><div class="line">			<span class="keyword">if</span>(divisor&gt;<span class="number">0</span>)</div><div class="line">				<span class="keyword">while</span>(dividend+divisor&lt;=<span class="number">0</span>)&#123;</div><div class="line">					dividend = dividend+divisor;</div><div class="line">					res--;</div><div class="line">				&#125;</div><div class="line">			<span class="keyword">else</span></div><div class="line">				<span class="keyword">while</span>(dividend-divisor&lt;=<span class="number">0</span>)&#123;</div><div class="line">					dividend = dividend-divisor;</div><div class="line">					res++;</div><div class="line">				&#125;</div><div class="line">		<span class="keyword">return</span> res;</div></pre></td></tr></table></figure>
<p>不停的减，在被除数很大的时候会时间越界。</p>
<p>想到《算法》书里通过</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Pow(<span class="keyword">int</span> x,<span class="keyword">int</span> N)&#123;</div><div class="line">  <span class="keyword">if</span>(N==<span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">  <span class="keyword">if</span>(IsEven(N))</div><div class="line">    <span class="keyword">return</span> Pow(x*x,N/<span class="number">2</span>);</div><div class="line">  <span class="keyword">else</span></div><div class="line">    <span class="keyword">return</span> Pow(x*x,N/<span class="number">2</span>)*x;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><a href="http://jlearning.cn/2017/03/23/%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E3%80%8B%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/" target="_blank" rel="external">的方式求幂运算</a>想这里能不能也这样用。相比较$pow(x*x,N/2)=pow(x,N)$的方式进行转换，除法$X/Y=2X/2Y$好像在这里没用。</p>
<hr>
<p>找到被除数中最大2的幂，然后迭代：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span>((mi+mi)&lt;=dividend)&#123;</div><div class="line">  mi = mi+mi;</div><div class="line">  d = d+d;  </div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> d+divide(dividend-mi,divisor)</div></pre></td></tr></table></figure>
<p>相当于转化为二进制的形式。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          
    
        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Zhuo Jimmy" />
          <p class="site-author-name" itemprop="name">Zhuo Jimmy</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">48</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/2116491901" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:zhuo_jimmy@yeah.net" target="_blank" title="email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  email
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuo Jimmy</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  



  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("HFHaSh6qpS4jVESzHUkGA3wi-gzGzoHsz", "5zRJEDM6Mbkh4tIGz0kaI6Vl");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  


</body>
</html>
